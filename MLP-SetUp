import numpy as np
import random
from sklearn.datasets import make_classification, make_moons, make_circles, load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split

class Activation:
    @staticmethod
    def relu(x):
        return np.maximum(0, x)

    @staticmethod
    def relu_derivative(x):
        return (x > 0).astype(float)

    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def sigmoid_derivative(x):
        s = Activation.sigmoid(x)
        return s * (1 - s)

    @staticmethod
    def softmax(x):
        # numerical stability
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

class Loss:
    @staticmethod
    def categorical_crossentropy(y_true, y_pred):
        eps = 1e-9
        y_pred = np.clip(y_pred, eps, 1 - eps)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

    @staticmethod
    def softmax_crossentropy_derivative(y_true, y_pred):
        # IMPORTANT RESULT:
        # dL/dZ = y_pred - y_true
        return (y_pred - y_true) / y_true.shape[0]


class Dense:
    def __init__(self, X, input_size, output_size, activation=None):
        self.special_weight = SpecialWeight(input_size, output_size)
        self.W = self.special_weight.weight_encoder(X)
        self.b = np.zeros((1, output_size))
        self.activation_name = activation

        if activation:
            self.activation = getattr(Activation, activation)
            self.activation_derivative = getattr(Activation, activation + "_derivative")
        else:
            self.activation = None
            self.activation_derivative = None


    def forward(self, x):
        self.x = x
        self.z = np.dot(x, self.W) + self.b

        if self.activation:
            self.a = self.activation(self.z)
        else:
            self.a = self.z

        return self.a


    def backward(self, da, lr):
        if self.activation_derivative:
            dz = da * self.activation_derivative(self.z)
        else:
            dz = da

        dW = np.dot(self.x.T, dz)
        db = np.sum(dz, axis=0, keepdims=True)
        dx = np.dot(dz, self.W.T)

        self.W -= lr * dW
        self.b -= lr * db

        return dx

class SoftmaxOutput:
    def forward(self, x):
        self.out = Activation.softmax(x)
        return self.out

    def backward(self, dL_dZ):
        # gradient already computed as y_pred - y_true
        return dL_dZ


class MLP:
    def __init__(self):
        self.layers = []
        self.softmax = SoftmaxOutput()
        self.acc = []

    def add(self, layer):
        self.layers.append(layer)

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return self.softmax.forward(x)

    def predict(self, X, y, epochs=1000, verbose=True):
    	for epoch in range(epochs):
    		y_pred = self.forward(X)
    		loss = Loss.categorical_crossentropy(y, y_pred)    		
    		if verbose and epoch % 100 == 0:
    		   acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
    		   self.acc2.append(acc)
    		   print(f"Epoch {epoch} | loss:{loss:.4f} | Acc: {acc:.2f}")    			  	

    def backward(self, grad, lr):
        grad = self.softmax.backward(grad)
        for layer in reversed(self.layers):
            grad = layer.backward(grad, lr)

    def prediction(self, X):
         y_pred = self.forward(X)   
         return y_pred   
            
    def score(self, X, y):
        y_pred = self.prediction(X)
        acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
        return acc         

    def train(self, X, y, epochs=1000, lr=0.01, verbose=True):
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = Loss.categorical_crossentropy(y, y_pred)
            grad = Loss.softmax_crossentropy_derivative(y, y_pred)
            self.backward(grad, lr)

            if verbose and epoch % 100 == 0:
                acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
                print(f"Epoch {epoch} | Loss: {loss:.4f} | Acc: {acc:.2f}")
                self.acc.append(acc)


# double test sets
def test_sets(type=None):
    if type == "moons":
    	X, y_raw = make_moons(
    	n_samples = 1000, 
    	noise=0.75,  
    	random_state=99)
    else:
    	X, y_raw = make_classification(
    	n_samples=1000,
    	n_features=3,
    	n_classes=3,
    	n_informative=3,
    	n_redundant=0,
    	class_sep=1.5,
    	random_state=99
    	)
    	
    return X, y_raw

def add_distribution_shift(X_test, severity='medium'):
    """Add realistic distribution shifts"""
    X_shifted = X_test.copy()
    n_features = X_test.shape[1]
    n_corrupt = min(3, n_features)
    
    corrupt_cols = np.random.choice(n_features, size=n_corrupt, replace=False)
    
    # Manual range calculation
    feature_ranges = X_test.max(axis=0) - X_test.min(axis=0)
    
    # Standard deviation (using numpy)
    feature_stds = np.std(X_test, axis=0)
    
    # Handle constant features (range or std = 0)
    feature_stds = np.where(feature_stds == 0, 1.0, feature_stds)
    feature_ranges = np.where(feature_ranges == 0, 1.0, feature_ranges)
    
    # Severity levels
    if severity == 'light':
        noise_factor = 0.1
    elif severity == 'medium':
        noise_factor = 0.3
    else:  # severe
        noise_factor = 0.5
    
    for col in corrupt_cols:
        # Use whichever scale makes sense for your data
        # Option A: Based on standard deviation
        noise = np.random.randn(len(X_test)) * feature_stds[col] * noise_factor
        
        # Option B: Based on range (uncomment if preferred)
        # noise = np.random.randn(len(X_test)) * feature_ranges[col] * noise_factor * 0.1
        
        X_shifted[:, col] += noise
    
    return X_shifted


# functions that test generalizations of accross inputs.

def realistic_model_test(model, X, y, n_trials=10):
    """Test model across multiple real-world scenarios"""
    results = {
        'standard_split': [],
        'noise_injected': [],
        'distribution_shift': []
    }
    
    for i in range(n_trials):
        # Standard split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.9, random_state=10*i
        )
 
        model.train(X_train, y_train, epochs=1000, lr=0.1)
        results['standard_split'].append(model.score(X_test, y_test))
      
        
        # Noise injected
        noise_level = np.random.uniform(0, 0.2)
        X_test_noisy = X_test + np.random.normal(0, noise_level, X_test.shape)
        results['noise_injected'].append(model.score(X_test_noisy, y_test))
        
        # Feature corruption (simulate sensor failure)
        X_test_shifted = add_distribution_shift(X_test, severity='medium')
        results['distribution_shift'].append(model.score(X_test_shifted, y_test))
    
    # Print results
    for scenario, scores in results.items():
        print(f"\n{scenario}:")
        print(f"  Mean: {np.mean(scores):.3f}")
        print(f"  Std:  {np.std(scores):.3f}")
        print(f"  Min:  {np.min(scores):.3f}")
        print(f"  Max:  {np.max(scores):.3f}")
    
    return results


X, y_raw = test_sets(type="make_class")

num_classes= len(np.unique(y_raw))
y = np.zeros((y_raw.size, num_classes))
y[np.arange(y_raw.size), y_raw] = 1

input_dim = X.shape[1]
output_dim = y.shape[1]


# regular trainings
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=99)
_, X_test2, _, y_test2 = train_test_split(X, y, test_size=0.9, random_state=123)


def add_noise(X, noise_level=0.1):
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

# optional corruptions you can try
X_noisy = add_noise(X_train, noise_level=0.9)  
y_noisy = add_noise(y_train, noise_level=0.9)  
X_noise_tested = add_noise(X_test, noise_level=0.9)
model = MLP()

model.add(Dense(X_train, input_dim, 100, activation="relu"))
model.add(Dense(X_train, 100, output_dim, activation='relu')) 

# realistic test sets.
results = realistic_model_test(model, X, y, n_trials=10)


base_trainings_accuracy = model.train(X_noisy, y_train, epochs=1000, lr=0.1) # with corruption of 90% noise of X_train samples. 1000 epochs trainings.


# tests accuracies
test_accuracy = model.predict(X_test, y_test, epochs=1000)
test_accuracy = model.predict(X_test2, y_test2, epochs=1000)


# complementary datas.
mean_accuracy = np.mean(model.acc)
std_accuracy = np.std(model.acc)
var_accuracy = np.var(model.acc)

print('mean Accuracy', mean_accuracy)
print('Std accuracy', std_accuracy)
print('var accuracy', var_accuracy)

       