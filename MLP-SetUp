import numpy as np
import random
from sklearn.datasets import make_classification, make_moons, make_circles, load_breast_cancer, load_digits
from sklearn.model_selection import train_test_split

class Activation:
    @staticmethod
    def relu(x):
        return np.maximum(0, x)

    @staticmethod
    def relu_derivative(x):
        return (x > 0).astype(float)

    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

    @staticmethod
    def sigmoid_derivative(x):
        s = Activation.sigmoid(x)
        return s * (1 - s)

    @staticmethod
    def softmax(x):
        # numerical stability
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

class Loss:
    @staticmethod
    def categorical_crossentropy(y_true, y_pred):
        eps = 1e-9
        y_pred = np.clip(y_pred, eps, 1 - eps)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))

    @staticmethod
    def softmax_crossentropy_derivative(y_true, y_pred):
        # IMPORTANT RESULT:
        # dL/dZ = y_pred - y_true
        return (y_pred - y_true) / y_true.shape[0]


class Dense:
    def __init__(self, X, input_size, output_size, activation=None):
        self.special_weight = SpecialWeight(input_size, output_size)
        self.W = self.special_weight.weight_encoder(X)
        self.b = np.zeros((1, output_size))
        self.activation_name = activation

        if activation:
            self.activation = getattr(Activation, activation)
            self.activation_derivative = getattr(Activation, activation + "_derivative")
        else:
            self.activation = None
            self.activation_derivative = None


    def forward(self, x):
        self.x = x
        self.z = np.dot(x, self.W) + self.b

        if self.activation:
            self.a = self.activation(self.z)
        else:
            self.a = self.z

        return self.a


    def backward(self, da, lr):
        if self.activation_derivative:
            dz = da * self.activation_derivative(self.z)
        else:
            dz = da

        dW = np.dot(self.x.T, dz)
        db = np.sum(dz, axis=0, keepdims=True)
        dx = np.dot(dz, self.W.T)

        self.W -= lr * dW
        self.b -= lr * db

        return dx

class SoftmaxOutput:
    def forward(self, x):
        self.out = Activation.softmax(x)
        return self.out

    def backward(self, dL_dZ):
        # gradient already computed as y_pred - y_true
        return dL_dZ


class MLP:
    def __init__(self):
        self.layers = []
        self.softmax = SoftmaxOutput()
        self.acc = []

    def add(self, layer):
        self.layers.append(layer)

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return self.softmax.forward(x)

    def backward(self, grad, lr):
        grad = self.softmax.backward(grad)
        for layer in reversed(self.layers):
            grad = layer.backward(grad, lr)

    def train(self, X, y, epochs=1000, lr=0.01, verbose=True):
        for epoch in range(epochs):
            y_pred = self.forward(X)
            loss = Loss.categorical_crossentropy(y, y_pred)
            grad = Loss.softmax_crossentropy_derivative(y, y_pred)
            self.backward(grad, lr)

            if verbose and epoch % 100 == 0:
                acc = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))
                print(f"Epoch {epoch} | Loss: {loss:.4f} | Acc: {acc:.2f}")
                self.acc.append(acc)


# double test sets
def test_sets(type=None):
    if type == "moons":
    	X, y_raw = make_moons(
    	n_samples = 1000, 
    	noise=0.75,  
    	random_state=99)
    else:
    	X, y_raw = make_classification(
    	n_samples=1000,
    	n_features=3,
    	n_classes=3,
    	n_informative=3,
    	n_redundant=0,
    	class_sep=1.5,
    	random_state=99
    	)
    	
    return X, y_raw

X, y_raw = test_sets(type="make_class")

num_classes= len(np.unique(y_raw))
y = np.zeros((y_raw.size, num_classes))
y[np.arange(y_raw.size), y_raw] = 1

input_dim = X.shape[1]
output_dim = y.shape[1]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=99)

def add_noise(X, noise_level=0.1):
    noise = np.random.normal(0, noise_level, X.shape)
    return X + noise

X_noisy = add_noise(X_train, noise_level=0.9)  
y_noisy = add_noise(y_train, noise_level=0.9)  
X_noise_tested = add_noise(X_test, noise_level=0.9)
model = MLP()

model.add(Dense(X_train, input_dim, 100, activation="relu"))
model.add(Dense(X_train, 100, output_dim, activation='relu')) 


base_trainings_accuracy = model.train(X_noisy, y_train, epochs=1000, lr=0.1) # with corruption of 90% noise of X_train samples. 1000 epochs trainings.

base_tests_accuracy = model.train(X_noise_test, y_test, epochs=1000, lr=0.1) # with corruption of 90% noise of X_test samples. 1000 epochs tests.


# complementary datas.
mean_accuracy = np.mean(model.acc)
std_accuracy = np.std(model.acc)
var_accuracy = np.var(model.acc)


print('mean Accuracy', mean_accuracy)
print('Std accuracy', std_accuracy)
print('var accuracy', var_accuracy)

       